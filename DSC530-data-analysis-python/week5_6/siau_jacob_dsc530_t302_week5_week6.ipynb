{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZp4Be3z4eJA"
      },
      "source": [
        "# DSC530-T302\n",
        "# Weeks 5-6 Assignment\n",
        "# Jacob Siau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W58yhTdztMCt"
      },
      "source": [
        "## Chapter 4 Exercises 1-2, 4, 6-7, 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1.\n",
        "\n",
        "With the earthquakes.csv file, select all the earthquakes in Japan with a magnitude of 4.9 or greater using the mb magnitude type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      mag magType           time                         place  tsunami  \\\n",
            "1563  4.9      mb  1538977532250  293km ESE of Iwo Jima, Japan        0   \n",
            "2576  5.4      mb  1538697528010    37km E of Tomakomai, Japan        0   \n",
            "3072  4.9      mb  1538579732490     15km ENE of Hasaki, Japan        0   \n",
            "3632  4.9      mb  1538450871260    53km ESE of Hitachi, Japan        0   \n",
            "\n",
            "     parsed_place  \n",
            "1563        Japan  \n",
            "2576        Japan  \n",
            "3072        Japan  \n",
            "3632        Japan  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the earthquakes data\n",
        "df = pd.read_csv('earthquakes.csv')\n",
        "\n",
        "# Select all earthquakes in Japan with a magnitude of 4.9 or greater using the mb magnitude type\n",
        "japan_earthquakes = df[(df['parsed_place'] == 'Japan') & (df['mag'] >= 4.9) & (df['magType'] == 'mb')]\n",
        "print(japan_earthquakes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2. \n",
        "Create bins for each full number of earthquake magnitude (for instance, the first bin is (0,1], the second is (1,2], and so on) with the ml magnitude type and count how many are in each bin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Earthquake counts by magnitude bin (ml type):\n",
            "mag\n",
            "(0, 1]    2207\n",
            "(1, 2]    3105\n",
            "(2, 3]     862\n",
            "(3, 4]     122\n",
            "(4, 5]       2\n",
            "(5, 6]       1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create bins for each full number of earthquake magnitude with ml magnitude type\n",
        "ml_earthquakes = df[df['magType'] == 'ml']\n",
        "\n",
        "# Create bins from 0 to the max magnitude (rounded up)\n",
        "bins = range(0, int(ml_earthquakes['mag'].max()) + 2)\n",
        "\n",
        "# Count earthquakes in each bin\n",
        "magnitude_bins = pd.cut(ml_earthquakes['mag'], bins=bins)\n",
        "bin_counts = magnitude_bins.value_counts().sort_index()\n",
        "\n",
        "print(\"Earthquake counts by magnitude bin (ml type):\")\n",
        "print(bin_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4.\n",
        "Build a crosstab with the earthquake data between the tsunami column and the magType column. Rather than showing the frequency count, show the maximum magnitude that was observed for each combination. Put the magnitude type along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crosstab of tsunami vs magType (showing max magnitude):\n",
            "magType   mb  mb_lg    md   mh   ml  ms_20    mw  mwb  mwr  mww\n",
            "tsunami                                                        \n",
            "0        5.6    3.5  4.11  1.1  4.2    NaN  3.83  5.8  4.8  6.0\n",
            "1        6.1    NaN   NaN  NaN  5.1    5.7  4.41  NaN  NaN  7.5\n"
          ]
        }
      ],
      "source": [
        "# Build a crosstab between tsunami and magType columns showing maximum magnitude\n",
        "crosstab = pd.crosstab(df['tsunami'], df['magType'], values=df['mag'], aggfunc='max')\n",
        "print(\"Crosstab of tsunami vs magType (showing max magnitude):\")\n",
        "print(crosstab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.\n",
        "Create a pivot table of the FAANG data that compares the stocks. Put the ticker in the rows and show the averages of the OHLC and volume traded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pivot table of FAANG data (averages of OHLC and volume):\n",
            "              close         high          low         open        volume\n",
            "ticker                                                                  \n",
            "AAPL      47.263357    47.748526    46.795877    47.277859  1.360803e+08\n",
            "AMZN    1641.726176  1662.839839  1619.840519  1644.072709  5.648994e+06\n",
            "FB       171.510956   173.613347   169.303148   171.472948  2.765860e+07\n",
            "GOOG    1113.225134  1125.777606  1101.001658  1113.554101  1.741965e+06\n",
            "NFLX     319.290319   325.219322   313.187330   319.620558  1.146962e+07\n"
          ]
        }
      ],
      "source": [
        "# Load the FAANG data\n",
        "faang_df = pd.read_csv('faang.csv')\n",
        "\n",
        "# Create a pivot table with ticker in rows and averages of OHLC and volume\n",
        "pivot_table = faang_df.pivot_table(\n",
        "    index='ticker',\n",
        "    values=['open', 'high', 'low', 'close', 'volume'],\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "print(\"Pivot table of FAANG data (averages of OHLC and volume):\")\n",
        "print(pivot_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 10.\n",
        "The European Centre for Disease Prevention and Control (ECDC) provides an open dataset on COVID-19 cases called daily number of new reported cases of COVID-19 by country worldwide. This dataset is updated daily, but we will use a snapshot that contains data through September 18, 2020. Complete the following tasks to practice the skills you've learned up to this point in the book:\n",
        "\n",
        "Prepare the data\n",
        "Read in the data in the covid19_cases.csv file.\n",
        "Create a date column by parsing the dateRep column into a datetime.\n",
        "Set the date column as the index.\n",
        "Use the replace() method to update all occurrences of United_States_of_America and United_Kingdom to USA and UK, respectively.\n",
        "Sort the index.\n",
        "For the five countries with the most cases (cumulative), find the day with the largest number of cases.\n",
        "Find the 7-day average change in COVID-19 cases for the last week in the data for the five countries with the most cases.\n",
        "Find the first date that each country other than China had cases.\n",
        "Rank the countries by cumulative cases using percentiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 countries with most cases: ['USA', 'India', 'Brazil', 'Russia', 'Peru']\n",
            "\n",
            "Day with largest number of cases for top 5 countries:\n",
            "USA: 2020-07-25 with 78427 cases\n",
            "India: 2020-09-17 with 97894 cases\n",
            "Brazil: 2020-07-30 with 69074 cases\n",
            "Russia: 2020-07-18 with 12640 cases\n",
            "Peru: 2020-08-17 with 10143 cases\n",
            "\n",
            "7-day average change in COVID-19 cases for the last week:\n",
            "USA: 1564.83\n",
            "India: -172.50\n",
            "Brazil: 1045.67\n",
            "Russia: 69.50\n",
            "Peru: 25.33\n",
            "\n",
            "First date each country (other than China) had cases:\n",
            "countriesAndTerritories\n",
            "Thailand      2020-01-13\n",
            "Japan         2020-01-15\n",
            "South_Korea   2020-01-20\n",
            "Taiwan        2020-01-21\n",
            "USA           2020-01-21\n",
            "Singapore     2020-01-24\n",
            "Vietnam       2020-01-24\n",
            "Malaysia      2020-01-25\n",
            "Nepal         2020-01-25\n",
            "Australia     2020-01-25\n",
            "dtype: datetime64[ns]\n",
            "\n",
            "Country rankings by cumulative cases (percentiles):\n",
            "countriesAndTerritories\n",
            "USA             100.000000\n",
            "India            99.523810\n",
            "Brazil           99.047619\n",
            "Russia           98.571429\n",
            "Peru             98.095238\n",
            "Colombia         97.619048\n",
            "Mexico           97.142857\n",
            "South_Africa     96.666667\n",
            "Spain            96.190476\n",
            "Argentina        95.714286\n",
            "Name: cases, dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_7384\\3061757763.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  first_cases = covid_df[covid_df['countriesAndTerritories'] != 'China'].groupby('countriesAndTerritories').apply(\n"
          ]
        }
      ],
      "source": [
        "# Load the COVID-19 data\n",
        "covid_df = pd.read_csv('covid19_cases.csv')\n",
        "\n",
        "# Create a date column by parsing the dateRep column into a datetime\n",
        "covid_df['date'] = pd.to_datetime(covid_df['dateRep'], format='%d/%m/%Y')\n",
        "\n",
        "# Set the date column as the index\n",
        "covid_df.set_index('date', inplace=True)\n",
        "\n",
        "# Replace country names\n",
        "covid_df['countriesAndTerritories'] = covid_df['countriesAndTerritories'].replace({\n",
        "    'United_States_of_America': 'USA',\n",
        "    'United_Kingdom': 'UK'\n",
        "})\n",
        "\n",
        "# Sort the index\n",
        "covid_df.sort_index(inplace=True)\n",
        "\n",
        "# Find the five countries with the most cumulative cases\n",
        "top_5_countries = covid_df.groupby('countriesAndTerritories')['cases'].sum().nlargest(5).index.tolist()\n",
        "print(\"Top 5 countries with most cases:\", top_5_countries)\n",
        "\n",
        "# For the five countries with the most cases, find the day with the largest number of cases\n",
        "print(\"\\nDay with largest number of cases for top 5 countries:\")\n",
        "for country in top_5_countries:\n",
        "    country_data = covid_df[covid_df['countriesAndTerritories'] == country]\n",
        "    max_cases_day = country_data['cases'].idxmax()\n",
        "    max_cases = country_data.loc[max_cases_day, 'cases']\n",
        "    print(f\"{country}: {max_cases_day.date()} with {max_cases} cases\")\n",
        "\n",
        "# Find the 7-day average change in COVID-19 cases for the last week for top 5 countries\n",
        "print(\"\\n7-day average change in COVID-19 cases for the last week:\")\n",
        "for country in top_5_countries:\n",
        "    country_data = covid_df[covid_df['countriesAndTerritories'] == country].sort_index()\n",
        "    last_week = country_data.tail(7)\n",
        "    avg_change = last_week['cases'].diff().mean()\n",
        "    print(f\"{country}: {avg_change:.2f}\")\n",
        "\n",
        "# Find the first date that each country other than China had cases\n",
        "print(\"\\nFirst date each country (other than China) had cases:\")\n",
        "first_cases = covid_df[covid_df['countriesAndTerritories'] != 'China'].groupby('countriesAndTerritories').apply(\n",
        "    lambda x: x[x['cases'] > 0].index.min()\n",
        ").dropna().sort_values()\n",
        "print(first_cases.head(10))\n",
        "\n",
        "# Rank the countries by cumulative cases using percentiles\n",
        "cumulative_cases = covid_df.groupby('countriesAndTerritories')['cases'].sum()\n",
        "percentile_ranks = cumulative_cases.rank(pct=True) * 100\n",
        "print(\"\\nCountry rankings by cumulative cases (percentiles):\")\n",
        "print(percentile_ranks.sort_values(ascending=False).head(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
