{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZp4Be3z4eJA"
      },
      "source": [
        "# DSC530-T302\n",
        "# Week 1-2 Assignment\n",
        "# Jacob Siau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W58yhTdztMCt"
      },
      "source": [
        "## Chapter 3 Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1.\n",
        "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file (obtained using the stock_analysis package we will build in Chapter 7, Financial Analysis – Bitcoin and the Stock Market). \n",
        "Combine them into a single file and store the dataframe of the FAANG data as faang for the rest of the exercises:\n",
        "Read in the aapl.csv, amzn.csv, fb.csv, goog.csv, and nflx.csv files.\n",
        "Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example); this is how you look up a stock. In this case, the filenames happen to be the ticker symbols.\n",
        "Append them together into a single dataframe.\n",
        "Save the result in a CSV file called faang.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "4PN_NYNL5s0g"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "5x6rbXPxaBw4",
        "outputId": "12c9cb23-d115-4531-a8f8-0611b7c61ded"
      },
      "outputs": [],
      "source": [
        "# read in the data from the csv files\n",
        "aapl = pd.read_csv('AAPL.csv')\n",
        "goog = pd.read_csv('GOOG.csv')\n",
        "amzn = pd.read_csv('AMZN.csv')\n",
        "nflx = pd.read_csv('NFLX.csv')\n",
        "fb = pd.read_csv('FB.csv')\n",
        "\n",
        "# add a column to each df\n",
        "aapl['ticker'] = 'AAPL'\n",
        "goog['ticker'] = 'GOOG'\n",
        "amzn['ticker'] = 'AMZN'\n",
        "nflx['ticker'] = 'NFLX'\n",
        "fb['ticker'] = 'FB'\n",
        "\n",
        "# concatenate the dataframes into a single dataframe\n",
        "stocks = pd.concat([aapl, goog, amzn, nflx, fb], ignore_index=True)\n",
        "\n",
        "# save the combined dataframe to a new csv file\n",
        "stocks.to_csv('faang.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.\n",
        "\n",
        "With faang, use type conversion to cast the values of the date column into datetimes and the volume column into integers. Then, sort by date and ticker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>43.075001</td>\n",
              "      <td>42.314999</td>\n",
              "      <td>42.540001</td>\n",
              "      <td>43.064999</td>\n",
              "      <td>102223600</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>1190.000000</td>\n",
              "      <td>1170.510010</td>\n",
              "      <td>1172.000000</td>\n",
              "      <td>1189.010010</td>\n",
              "      <td>2694500</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>181.580002</td>\n",
              "      <td>177.550003</td>\n",
              "      <td>177.679993</td>\n",
              "      <td>181.419998</td>\n",
              "      <td>18151900</td>\n",
              "      <td>FB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>1066.939941</td>\n",
              "      <td>1045.229980</td>\n",
              "      <td>1048.339966</td>\n",
              "      <td>1065.000000</td>\n",
              "      <td>1237600</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>201.649994</td>\n",
              "      <td>195.419998</td>\n",
              "      <td>196.100006</td>\n",
              "      <td>201.070007</td>\n",
              "      <td>10966900</td>\n",
              "      <td>NFLX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>43.637501</td>\n",
              "      <td>42.990002</td>\n",
              "      <td>43.132500</td>\n",
              "      <td>43.057499</td>\n",
              "      <td>118071600</td>\n",
              "      <td>AAPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>1205.489990</td>\n",
              "      <td>1188.300049</td>\n",
              "      <td>1188.300049</td>\n",
              "      <td>1204.199951</td>\n",
              "      <td>3108800</td>\n",
              "      <td>AMZN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>184.779999</td>\n",
              "      <td>181.330002</td>\n",
              "      <td>181.880005</td>\n",
              "      <td>184.669998</td>\n",
              "      <td>16886600</td>\n",
              "      <td>FB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>1086.290039</td>\n",
              "      <td>1063.209961</td>\n",
              "      <td>1064.310059</td>\n",
              "      <td>1082.479980</td>\n",
              "      <td>1430200</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>206.210007</td>\n",
              "      <td>201.500000</td>\n",
              "      <td>202.050003</td>\n",
              "      <td>205.050003</td>\n",
              "      <td>8591400</td>\n",
              "      <td>NFLX</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           date         high          low         open        close  \\\n",
              "0    2018-01-02    43.075001    42.314999    42.540001    43.064999   \n",
              "502  2018-01-02  1190.000000  1170.510010  1172.000000  1189.010010   \n",
              "1004 2018-01-02   181.580002   177.550003   177.679993   181.419998   \n",
              "251  2018-01-02  1066.939941  1045.229980  1048.339966  1065.000000   \n",
              "753  2018-01-02   201.649994   195.419998   196.100006   201.070007   \n",
              "1    2018-01-03    43.637501    42.990002    43.132500    43.057499   \n",
              "503  2018-01-03  1205.489990  1188.300049  1188.300049  1204.199951   \n",
              "1005 2018-01-03   184.779999   181.330002   181.880005   184.669998   \n",
              "252  2018-01-03  1086.290039  1063.209961  1064.310059  1082.479980   \n",
              "754  2018-01-03   206.210007   201.500000   202.050003   205.050003   \n",
              "\n",
              "         volume ticker  \n",
              "0     102223600   AAPL  \n",
              "502     2694500   AMZN  \n",
              "1004   18151900     FB  \n",
              "251     1237600   GOOG  \n",
              "753    10966900   NFLX  \n",
              "1     118071600   AAPL  \n",
              "503     3108800   AMZN  \n",
              "1005   16886600     FB  \n",
              "252     1430200   GOOG  \n",
              "754     8591400   NFLX  "
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cast the values of the date column into datetimes and the volume column into integers\n",
        "faang_df = pd.read_csv('faang.csv')\n",
        "faang_df['date'] = pd.to_datetime(faang_df['date'])\n",
        "faang_df['volume'] = faang_df['volume'].astype(int)\n",
        "\n",
        "# sort by date and ticket\n",
        "faang_df = faang_df.sort_values(by=['date', 'ticker'])\n",
        "# show the sorting worked\n",
        "faang_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.\n",
        "Find the seven rows in faang with the lowest value for volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>2018-07-03</td>\n",
              "      <td>1135.819946</td>\n",
              "      <td>1100.020020</td>\n",
              "      <td>1135.819946</td>\n",
              "      <td>1102.890015</td>\n",
              "      <td>679000</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>2018-11-23</td>\n",
              "      <td>1037.589966</td>\n",
              "      <td>1022.398987</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>1023.880005</td>\n",
              "      <td>691500</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>2018-05-24</td>\n",
              "      <td>1080.469971</td>\n",
              "      <td>1066.150024</td>\n",
              "      <td>1079.000000</td>\n",
              "      <td>1079.239990</td>\n",
              "      <td>766800</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>2018-07-10</td>\n",
              "      <td>1159.589966</td>\n",
              "      <td>1149.589966</td>\n",
              "      <td>1156.979980</td>\n",
              "      <td>1152.839966</td>\n",
              "      <td>798400</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>2018-08-09</td>\n",
              "      <td>1255.541992</td>\n",
              "      <td>1246.010010</td>\n",
              "      <td>1249.900024</td>\n",
              "      <td>1249.099976</td>\n",
              "      <td>848600</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>2018-08-20</td>\n",
              "      <td>1211.000000</td>\n",
              "      <td>1194.625977</td>\n",
              "      <td>1205.020020</td>\n",
              "      <td>1207.770020</td>\n",
              "      <td>870800</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>2018-08-22</td>\n",
              "      <td>1211.839966</td>\n",
              "      <td>1199.000000</td>\n",
              "      <td>1200.000000</td>\n",
              "      <td>1207.329956</td>\n",
              "      <td>887400</td>\n",
              "      <td>GOOG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          date         high          low         open        close  volume  \\\n",
              "377 2018-07-03  1135.819946  1100.020020  1135.819946  1102.890015  679000   \n",
              "477 2018-11-23  1037.589966  1022.398987  1030.000000  1023.880005  691500   \n",
              "350 2018-05-24  1080.469971  1066.150024  1079.000000  1079.239990  766800   \n",
              "381 2018-07-10  1159.589966  1149.589966  1156.979980  1152.839966  798400   \n",
              "403 2018-08-09  1255.541992  1246.010010  1249.900024  1249.099976  848600   \n",
              "410 2018-08-20  1211.000000  1194.625977  1205.020020  1207.770020  870800   \n",
              "412 2018-08-22  1211.839966  1199.000000  1200.000000  1207.329956  887400   \n",
              "\n",
              "    ticker  \n",
              "377   GOOG  \n",
              "477   GOOG  \n",
              "350   GOOG  \n",
              "381   GOOG  \n",
              "403   GOOG  \n",
              "410   GOOG  \n",
              "412   GOOG  "
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# find the seven rows in faang with the lowest value for volume\n",
        "faang_df.nsmallest(7, 'volume')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4.\n",
        "Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>variable</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>open</td>\n",
              "      <td>42.540001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>AMZN</td>\n",
              "      <td>open</td>\n",
              "      <td>1172.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>FB</td>\n",
              "      <td>open</td>\n",
              "      <td>177.679993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>GOOG</td>\n",
              "      <td>open</td>\n",
              "      <td>1048.339966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>NFLX</td>\n",
              "      <td>open</td>\n",
              "      <td>196.100006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>open</td>\n",
              "      <td>43.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>AMZN</td>\n",
              "      <td>open</td>\n",
              "      <td>1188.300049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>FB</td>\n",
              "      <td>open</td>\n",
              "      <td>181.880005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>GOOG</td>\n",
              "      <td>open</td>\n",
              "      <td>1064.310059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>NFLX</td>\n",
              "      <td>open</td>\n",
              "      <td>202.050003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date ticker variable        value\n",
              "0 2018-01-02   AAPL     open    42.540001\n",
              "1 2018-01-02   AMZN     open  1172.000000\n",
              "2 2018-01-02     FB     open   177.679993\n",
              "3 2018-01-02   GOOG     open  1048.339966\n",
              "4 2018-01-02   NFLX     open   196.100006\n",
              "5 2018-01-03   AAPL     open    43.132500\n",
              "6 2018-01-03   AMZN     open  1188.300049\n",
              "7 2018-01-03     FB     open   181.880005\n",
              "8 2018-01-03   GOOG     open  1064.310059\n",
              "9 2018-01-03   NFLX     open   202.050003"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Right now, the data is somewhere between long and wide format. \n",
        "# Use melt() to make it completely long format. \n",
        "# Hint: date and ticker are our ID variables (they uniquely identify each row). \n",
        "# We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume.\n",
        "\n",
        "# convert to long format using melt()\n",
        "faang_df_long = pd.melt(\n",
        "    faang_df, \n",
        "    id_vars=['date', 'ticker'], \n",
        "    value_vars=['open', 'high', 'low', 'close', 'volume'], \n",
        "    var_name='variable', \n",
        "    value_name='value'\n",
        ")\n",
        "\n",
        "# show the first few rows of the long format dataframe\n",
        "faang_df_long.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5.\n",
        "Suppose we found out that on July 26, 2018 there was a glitch in how the data was recorded. How should we handle this? Note that there is no coding required for this exercise.\n",
        "\n",
        "#### My answer\n",
        "We should remove the data for that date. The glitch means the data from that date is unreliable and it should be discarded. If needed, we can extrapolate through that date with linear regression; however, if permissible, we should simply remove that date. Any timeseries data analysis over a long enough period of time should not be impacted much by one missing date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.\n",
        "The European Centre for Disease Prevention and Control (ECDC) provides an open dataset on COVID-19 cases called daily number of new reported cases of COVID-19 by country worldwide. This dataset is updated daily, but we will use a snapshot that contains data from January 1, 2020 through September 18, 2020. Clean and pivot the data so that it is in wide format:\n",
        "\n",
        "Read in the covid19_cases.csv file.\n",
        "Create a date column using the data in the dateRep column and the pd.to_datetime() function.\n",
        "Set the date column as the index and sort the index.\n",
        "Replace all occurrences of United_States_of_America and United_Kingdom with USA and UK, respectively. Hint: the replace() method can be run on the dataframe as a whole.\n",
        "Using the countriesAndTerritories column, filter the cleaned COVID-19 cases data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA.\n",
        "Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts (the cases column). Be sure to fill in NaN values with 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>countriesAndTerritories</th>\n",
              "      <th>Argentina</th>\n",
              "      <th>Brazil</th>\n",
              "      <th>China</th>\n",
              "      <th>Colombia</th>\n",
              "      <th>India</th>\n",
              "      <th>Italy</th>\n",
              "      <th>Mexico</th>\n",
              "      <th>Peru</th>\n",
              "      <th>Russia</th>\n",
              "      <th>Spain</th>\n",
              "      <th>Turkey</th>\n",
              "      <th>UK</th>\n",
              "      <th>USA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-01-01</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-02</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-03</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-04</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-01-05</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-14</th>\n",
              "      <td>10778.0</td>\n",
              "      <td>14768.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>7355.0</td>\n",
              "      <td>92071.0</td>\n",
              "      <td>1456.0</td>\n",
              "      <td>4408.0</td>\n",
              "      <td>6787.0</td>\n",
              "      <td>5449.0</td>\n",
              "      <td>27404.0</td>\n",
              "      <td>1527.0</td>\n",
              "      <td>3330.0</td>\n",
              "      <td>33871.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-15</th>\n",
              "      <td>9056.0</td>\n",
              "      <td>15155.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>5573.0</td>\n",
              "      <td>83809.0</td>\n",
              "      <td>1008.0</td>\n",
              "      <td>3335.0</td>\n",
              "      <td>4241.0</td>\n",
              "      <td>5509.0</td>\n",
              "      <td>9437.0</td>\n",
              "      <td>1716.0</td>\n",
              "      <td>2621.0</td>\n",
              "      <td>34841.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-16</th>\n",
              "      <td>9908.0</td>\n",
              "      <td>36653.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>6698.0</td>\n",
              "      <td>90123.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>4771.0</td>\n",
              "      <td>4160.0</td>\n",
              "      <td>5529.0</td>\n",
              "      <td>11193.0</td>\n",
              "      <td>1742.0</td>\n",
              "      <td>3103.0</td>\n",
              "      <td>51473.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-17</th>\n",
              "      <td>11893.0</td>\n",
              "      <td>36820.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7787.0</td>\n",
              "      <td>97894.0</td>\n",
              "      <td>1452.0</td>\n",
              "      <td>4444.0</td>\n",
              "      <td>6380.0</td>\n",
              "      <td>5670.0</td>\n",
              "      <td>11291.0</td>\n",
              "      <td>1771.0</td>\n",
              "      <td>3991.0</td>\n",
              "      <td>24598.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-18</th>\n",
              "      <td>11674.0</td>\n",
              "      <td>36303.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>7568.0</td>\n",
              "      <td>96424.0</td>\n",
              "      <td>1583.0</td>\n",
              "      <td>3182.0</td>\n",
              "      <td>5698.0</td>\n",
              "      <td>5762.0</td>\n",
              "      <td>14389.0</td>\n",
              "      <td>1648.0</td>\n",
              "      <td>3395.0</td>\n",
              "      <td>43567.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>262 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "countriesAndTerritories  Argentina   Brazil  China  Colombia    India   Italy  \\\n",
              "date                                                                            \n",
              "2020-01-01                     0.0      0.0    0.0       0.0      0.0     0.0   \n",
              "2020-01-02                     0.0      0.0    0.0       0.0      0.0     0.0   \n",
              "2020-01-03                     0.0      0.0   17.0       0.0      0.0     0.0   \n",
              "2020-01-04                     0.0      0.0    0.0       0.0      0.0     0.0   \n",
              "2020-01-05                     0.0      0.0   15.0       0.0      0.0     0.0   \n",
              "...                            ...      ...    ...       ...      ...     ...   \n",
              "2020-09-14                 10778.0  14768.0   29.0    7355.0  92071.0  1456.0   \n",
              "2020-09-15                  9056.0  15155.0   22.0    5573.0  83809.0  1008.0   \n",
              "2020-09-16                  9908.0  36653.0   24.0    6698.0  90123.0  1229.0   \n",
              "2020-09-17                 11893.0  36820.0    7.0    7787.0  97894.0  1452.0   \n",
              "2020-09-18                 11674.0  36303.0   44.0    7568.0  96424.0  1583.0   \n",
              "\n",
              "countriesAndTerritories  Mexico    Peru  Russia    Spain  Turkey      UK  \\\n",
              "date                                                                       \n",
              "2020-01-01                  0.0     0.0     0.0      0.0     0.0     0.0   \n",
              "2020-01-02                  0.0     0.0     0.0      0.0     0.0     0.0   \n",
              "2020-01-03                  0.0     0.0     0.0      0.0     0.0     0.0   \n",
              "2020-01-04                  0.0     0.0     0.0      0.0     0.0     0.0   \n",
              "2020-01-05                  0.0     0.0     0.0      0.0     0.0     0.0   \n",
              "...                         ...     ...     ...      ...     ...     ...   \n",
              "2020-09-14               4408.0  6787.0  5449.0  27404.0  1527.0  3330.0   \n",
              "2020-09-15               3335.0  4241.0  5509.0   9437.0  1716.0  2621.0   \n",
              "2020-09-16               4771.0  4160.0  5529.0  11193.0  1742.0  3103.0   \n",
              "2020-09-17               4444.0  6380.0  5670.0  11291.0  1771.0  3991.0   \n",
              "2020-09-18               3182.0  5698.0  5762.0  14389.0  1648.0  3395.0   \n",
              "\n",
              "countriesAndTerritories      USA  \n",
              "date                              \n",
              "2020-01-01                   0.0  \n",
              "2020-01-02                   0.0  \n",
              "2020-01-03                   0.0  \n",
              "2020-01-04                   0.0  \n",
              "2020-01-05                   0.0  \n",
              "...                          ...  \n",
              "2020-09-14               33871.0  \n",
              "2020-09-15               34841.0  \n",
              "2020-09-16               51473.0  \n",
              "2020-09-17               24598.0  \n",
              "2020-09-18               43567.0  \n",
              "\n",
              "[262 rows x 13 columns]"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read in the covid19_cases.csv file.\n",
        "covid_df = pd.read_csv('covid19_cases.csv')\n",
        "\n",
        "# Create a date column using the data in the dateRep column and the pd.to_datetime() function.\n",
        "covid_df['date'] = pd.to_datetime(covid_df['dateRep'], format='%d/%m/%Y')\n",
        "\n",
        "# Set the date column as the index and sort the index.\n",
        "covid_df = covid_df.set_index('date').sort_index()\n",
        "\n",
        "# Replace all occurrences of United_States_of_America and United_Kingdom with USA and UK, respectively. \n",
        "# Hint: the replace() method can be run on the dataframe as a whole.\n",
        "covid_df = covid_df.replace({'United_States_of_America': 'USA', 'United_Kingdom': 'UK'})\n",
        "\n",
        "# Using the countriesAndTerritories column, filter the cleaned COVID-19 cases data down to Argentina, Brazil, China, Colombia, India, Italy, Mexico, Peru, Russia, Spain, Turkey, the UK, and the USA.\n",
        "countries = [\n",
        "    'Argentina', 'Brazil', 'China', 'Colombia', 'India', \n",
        "    'Italy', 'Mexico', 'Peru', 'Russia', 'Spain', \n",
        "    'Turkey', 'UK', 'USA'\n",
        "]\n",
        "covid_filtered = covid_df[covid_df['countriesAndTerritories'].isin(countries)]\n",
        "\n",
        "# Show the first few rows of the filtered dataframe\n",
        "covid_filtered.head()\n",
        "\n",
        "# Pivot the data so that the index contains the dates, the columns contain the country names, and the values are the case counts (the cases column). Be sure to fill in NaN values with 0.\n",
        "covid_pivot = covid_filtered.pivot_table(\n",
        "    index=covid_filtered.index, \n",
        "    columns='countriesAndTerritories', \n",
        "    values='cases', \n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "# Write the pivoted dataframe to a new CSV file named covid19_cases_cleaned.csv.\n",
        "covid_pivot.to_csv('covid19_cases_cleaned.csv')\n",
        "\n",
        "# Show the filtered dataframe\n",
        "covid_pivot"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
